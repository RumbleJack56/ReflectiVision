{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, scale_factor, mode):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=True)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def gridImage(imgs):\n",
    "\n",
    "    img_grid = make_grid(imgs[0], nrow=1, normalize=False)\n",
    "\n",
    "    for i in range(1,len(imgs)):\n",
    "        img_grid = torch.cat((img_grid, make_grid(imgs[i], nrow=1, normalize=False)), -1)\n",
    "\n",
    "    return img_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "arima losss type\n",
    "loss_edge_corr = criterion_edge_corr(reduce(lambda x,y:x*y,GCLoss(*net(Input)), 0)\n",
    "loss_edge_sum = criterion_edge_sum(gradB + gradR, imgs_grad_label)\n",
    "loss_edge = (epoch - 1) * (loss_edge_sum + loss_edge_corr)   \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCLoss, self).__init__()\n",
    "\n",
    "        #sobelfilters , lib not workign\n",
    "        sobel_x = torch.Tensor([[1,0,-1],[2,0,-2],[1,0,-1]]).view((1,1,3,3)).repeat(1,3,1,1)\n",
    "        sobel_y = torch.Tensor([[1,2,1],[0,0,0],[-1,-2,-1]]).view((1,1,3,3)).repeat(1,3,1,1)\n",
    "\n",
    "        self.gxb = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.gxb.weight = nn.Parameter(sobel_x)\n",
    "        for param in self.gxb.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.gyb = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.gyb.weight = nn.Parameter(sobel_y)\n",
    "        for param in self.gyb.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.gxr = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.gxr.weight = nn.Parameter(sobel_x)\n",
    "        for param in self.gxr.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.gyr = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.gyr.weight = nn.Parameter(sobel_y)\n",
    "        for param in self.gyr.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.af_B = nn.Tanhshrink()\n",
    "        self.af_R = nn.Tanhshrink()\n",
    "        \n",
    "\n",
    "    def forward(self, B, R):\n",
    "\n",
    "        gradout_B = self.af_B(self.gyb(B) + self.gxb(B))\n",
    "        gradout_R = self.af_R(self.gyr(R) + self.gxr(R))\n",
    "        return gradout_B, gradout_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#values in stateoftheart\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "padsize = 150\n",
    "\n",
    "class testImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        \n",
    "        self.tensor_setup = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.files = sorted(glob.glob(root + \"/input/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filePath = self.files[index % len(self.files)]\n",
    "        R = np.array(Image.open(filePath),'f') / 255.\n",
    "        R = np.pad(R,[(padsize,padsize),(padsize,padsize),(0,0)],'symmetric')\n",
    "\n",
    "        return {\"R\": self.tensor_setup(R[:,:,:3]), \"Name\": os.path.basename(filePath).split(\".\")[0]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "\n",
    "class gtTestImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        \n",
    "        self.tensor_setup = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "  \n",
    "        self.files_base = sorted(glob.glob(root + \"/gt/*.*\"))\n",
    "        self.files_input = sorted(glob.glob(root + \"/input/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filePath = self.files_base[index % len(self.files_base)]\n",
    "        B = np.array(Image.open(filePath),'f') /255.\n",
    "        R = np.array(Image.open(self.files_input[index % len(self.files_base)]),'f') / 255. \n",
    "        R = np.pad(R,[(padsize,padsize),(padsize,padsize),(0,0)],'symmetric')\n",
    "\n",
    "        return {\"R\": self.tensor_setup(R[:,:,:3]), \n",
    "                \n",
    "                \"B\": B[:,:,:3], \n",
    "                \"Name\": os.path.basename(filePath).split(\".\")[0]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, activation=nn.LeakyReLU(negative_slope=0.01,inplace=True)):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, middle_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            activation,\n",
    "            nn.Conv2d(middle_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation)\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "\n",
    "class GCNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
    "        super(GCNet, self).__init__()\n",
    "\n",
    "\n",
    "        filt = [32, 64, 128, 256, 512]   #  filtersssssss\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up = Interpolate(scale_factor=2, mode='bilinear')\n",
    "\n",
    "\n",
    "        # self.conv0_0 = ConvBlock(in_channels, filt[0], filt[0])\n",
    "        # self.conv1_0 = ConvBlocks(filt[0], filt[1], filt[1])\n",
    "        # self.conv2_0 = ConvBlock(filt[1], filt[2], filt[2])\n",
    "        # self.conv3_0 = ConvBlock(filt[2], filt[3], filt[3])\n",
    "        # self.conv4_0 = ConvBlock(filt[3], filt[4], filt[4])\n",
    "        \n",
    "        \n",
    "        self.conv0_0 = ConvBlock(in_channels, filt[0], filt[0])\n",
    "        self.conv1_0= ConvBlock(filt[0]  , filt[1], filt[1])\n",
    "        self.conv2_0= ConvBlock(filt[1],  filt[2], filt[2])\n",
    "        self.conv3_0 = ConvBlock(filt[2] , filt[3], filt[3])\n",
    "        self.conv4_0= ConvBlock( filt[3] , filt[4], filt[4])\n",
    "\n",
    "        self.conv0_1 = ConvBlock(filt[0]+filt[1], filt[0], filt[0])\n",
    "        self.conv1_1 = ConvBlock(filt[1]+filt[2], filt[1], filt[1])\n",
    "        # self.conv1_1 = ConvBlock(filt[1]+filt[2], filt[3], filt[3])\n",
    "        self.conv2_1 = ConvBlock(filt[2]+filt[3], filt[2], filt[2])\n",
    "        # self.conv2_1 = ConvBlock(filt[2]+filt[3], filt[3], filt[3])\n",
    "        self.conv3_1 = ConvBlock(filt[3]+filt[4], filt[3], filt[3])\n",
    "\n",
    "        self.conv0_2 = ConvBlock(filt[0]*2+filt[1], filt[0], filt[0])\n",
    "        self.conv1_2 = ConvBlock(filt[1]*2+filt[2], filt[1], filt[1])\n",
    "        self.conv2_2 = ConvBlock(filt[2]*2+filt[3], filt[2], filt[2])\n",
    "        # self.conv2_2 = ConvBlock(filt[2]*2+filt[3], filt[3], filt[3])\n",
    "\n",
    "\n",
    "        self.conv0_3 = ConvBlock(filt[0]*3+filt[1], filt[0], filt[0])\n",
    "        self.conv1_3 = ConvBlock(filt[1]*3+filt[2], filt[1], filt[1])\n",
    "\n",
    "        self.conv0_4 = ConvBlock(filt[0]*4+filt[1], filt[0], filt[0])\n",
    "\n",
    "        self.final1 = nn.Sequential(nn.Conv2d(filt[0], out_channels, kernel_size=3, padding=1))\n",
    "        self.final2 = nn.Sequential(nn.Conv2d(filt[0], out_channels, kernel_size=3, padding=1))\n",
    "        self.final3 = nn.Sequential(nn.Conv2d(filt[0], out_channels, kernel_size=3, padding=1))\n",
    "        self.final4 = nn.Sequential(nn.Conv2d(filt[0], filt[0], 5, padding=2),\n",
    "            nn.BatchNorm2d(filt[0]),\n",
    "            nn.LeakyReLU(negative_slope=0.01,inplace=True),\n",
    "            nn.Conv2d(filt[0], out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.G_x_D = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.G_y_D = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.G_x_G = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "        self.G_y_G = nn.Conv2d(3,1,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0_0 = self.conv0_0(x)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        output4 = self.final4(x0_4)\n",
    "\n",
    "        return output4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset name: test_dataset] --> 2 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def convert_to_numpy(input,H,W):\n",
    "    image = input[:,:,padsize:H-padsize,padsize:W-padsize].clone()\n",
    "    input_numpy = image[:,:,:H,:W].clone().cpu().numpy().reshape(3,H-padsize*2,W-padsize*2).transpose(1,2,0)\n",
    "    for i in range(3):\n",
    "        input_numpy[:,:,i] = input_numpy[:,:,i] * std[i] + mean[i]\n",
    "\n",
    "    return  input_numpy\n",
    "\n",
    "dataset = \"test_dataset\"\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"../.dataset/\" + dataset + \"/output\", exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available(): device , torch.backends.cudnn.benchmark = 'cuda' , True\n",
    "else:device = 'cpu'\n",
    "\n",
    "# Initialize generator\n",
    "Generator = GCNet().to(device)\n",
    "Generator.eval()\n",
    "Generator.load_state_dict(torch.load(\"../checkpoint/model_gc.pth\", map_location=device,weights_only=True))\n",
    "\n",
    "# read image\n",
    "gtAvailable = False\n",
    "if os.path.exists(\"../.dataset/\" + dataset + \"/gt\"):\n",
    "    if len(os.listdir(\"../.dataset/\" + dataset + \"/input\")) == len(os.listdir(\"../.dataset/\" + dataset + \"/gt\")):\n",
    "        gtAvailable = True\n",
    "\n",
    "if gtAvailable:\n",
    "    image_dataset = gtTestImageDataset(\"../.dataset/\" + dataset)\n",
    "else:\n",
    "    image_dataset = testImageDataset(\"../.dataset/\" + dataset)\n",
    "\n",
    "# run\n",
    "all_psnr = 0.0\n",
    "all_ssim = 0.0\n",
    "print(\"[Dataset name: %s] --> %d images\" % (dataset, len(image_dataset)))\n",
    "for image_num in tqdm(range(len(image_dataset)),ncols=100):\n",
    "\n",
    "    data = image_dataset[image_num]\n",
    "    R = data[\"R\"].to(device)\n",
    "\n",
    "    _,first_h,first_w = R.size()\n",
    "    R = torch.nn.functional.pad(R,(0,(R.size(2)//16)*16+16-R.size(2),0,(R.size(1)//16)*16+16-R.size(1)),\"constant\")\n",
    "    R = R.view(1,3,R.size(1),R.size(2))\n",
    "    with torch.no_grad():output  = Generator(R) \n",
    "\n",
    "    #output image\n",
    "    output_np = np.clip(convert_to_numpy(output,first_h,first_w) + 0.015,0,1)\n",
    "    R_np = convert_to_numpy(R,first_h,first_w)\n",
    "    final_output = np.fmin(output_np, R_np)\n",
    "\n",
    "    # save output\n",
    "    Image.fromarray(np.uint8(final_output * 255)).save(\"../.dataset/\" + dataset + \"/output/\" + data[\"Name\"] + \".png\")\n",
    "\n",
    "    # Calculate PSNR/SSIM if available\n",
    "    if gtAvailable:\n",
    "        B = data[\"B\"].astype(np.float32)\n",
    "        # print(B.shape,final_output.shape)\n",
    "        thisPSNR = psnr(B, final_output.astype(np.float32))\n",
    "        thisSSIM = ssim(B, final_output.astype(np.float32), multichannel=True, channel_axis=2,data_range=255.0)\n",
    "        all_psnr += thisPSNR\n",
    "        all_ssim += thisSSIM\n",
    "        print(\"[%s] PSNR:%4.2f SSIM:%4.3f\" % (data[\"Name\"], thisPSNR, thisSSIM), end=\"\\r\")\n",
    "\n",
    "if gtAvailable:\n",
    "    all_psnr = all_psnr/len(image_dataset)\n",
    "    all_ssim = all_ssim/len(image_dataset)\n",
    "    print(\"hogya : [[%s]]\" % (dataset))\n",
    "    print(\"PSNR: %4.2f / SSIM: %4.3f\" % (all_psnr, all_ssim))\n",
    "else:\n",
    "    print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
